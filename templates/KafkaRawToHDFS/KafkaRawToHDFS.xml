<?xml version="1.0" encoding="UTF-8" standalone="yes"?><template><description>Processor Group Template to pull from Kafka Topic continuously and write raw output to hdfs. This processor could be modified and updated to add any required metadata.</description><name>KafkaRawToHDFS</name><snippet><processGroups><id>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</id><parentGroupId>7c84501d-d10c-407c-b9f3-1d80e38fe36a</parentGroupId><position><x>13.563720703125</x><y>-76.5101318359375</y></position><activeRemotePortCount>0</activeRemotePortCount><comments>Processor Group Template to pull from Kafka Topic continuously and write raw output to hdfs. This processor could be modified and updated to add any required metadata.</comments><contents><connections><id>1290b2fe-332e-4947-8460-57181b01d493</id><parentGroupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</parentGroupId><backPressureDataSizeThreshold>0 MB</backPressureDataSizeThreshold><backPressureObjectThreshold>0</backPressureObjectThreshold><destination><groupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</groupId><id>f155df39-c188-43b1-bc24-a25145ed57df</id><type>PROCESSOR</type></destination><flowFileExpiration>0 sec</flowFileExpiration><labelIndex>1</labelIndex><name>
          </name><selectedRelationships>merged</selectedRelationships><source><groupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</groupId><id>298fca06-1fb3-441e-a155-f6074181d1cf</id><type>PROCESSOR</type></source><zIndex>0</zIndex></connections><connections><id>84983959-a32d-4ce5-abc7-768ee9b83a0d</id><parentGroupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</parentGroupId><backPressureDataSizeThreshold>0 MB</backPressureDataSizeThreshold><backPressureObjectThreshold>0</backPressureObjectThreshold><destination><groupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</groupId><id>079d846d-d951-4188-9613-03e38083e238</id><type>PROCESSOR</type></destination><flowFileExpiration>0 sec</flowFileExpiration><labelIndex>1</labelIndex><name>
          </name><selectedRelationships>success</selectedRelationships><source><groupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</groupId><id>f155df39-c188-43b1-bc24-a25145ed57df</id><type>PROCESSOR</type></source><zIndex>0</zIndex></connections><connections><id>67fc2080-2dec-4f90-8eb9-97d4d0775ade</id><parentGroupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</parentGroupId><backPressureDataSizeThreshold>0 MB</backPressureDataSizeThreshold><backPressureObjectThreshold>0</backPressureObjectThreshold><destination><groupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</groupId><id>298fca06-1fb3-441e-a155-f6074181d1cf</id><type>PROCESSOR</type></destination><flowFileExpiration>0 sec</flowFileExpiration><labelIndex>1</labelIndex><name></name><selectedRelationships>success</selectedRelationships><source><groupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</groupId><id>0aa72ec0-717f-4386-983f-73b8647ac810</id><type>PROCESSOR</type></source><zIndex>0</zIndex></connections><connections><id>717cc1b2-2f90-4662-b9fa-28a8a978457c</id><parentGroupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</parentGroupId><backPressureDataSizeThreshold>0 MB</backPressureDataSizeThreshold><backPressureObjectThreshold>0</backPressureObjectThreshold><destination><groupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</groupId><id>0aa72ec0-717f-4386-983f-73b8647ac810</id><type>PROCESSOR</type></destination><flowFileExpiration>0 sec</flowFileExpiration><labelIndex>1</labelIndex><name></name><selectedRelationships>success</selectedRelationships><source><groupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</groupId><id>a2df0212-3e79-4d31-8dfd-3271bef4cbe0</id><type>PROCESSOR</type></source><zIndex>0</zIndex></connections><processors><id>0aa72ec0-717f-4386-983f-73b8647ac810</id><parentGroupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</parentGroupId><position><x>-395.9447668290362</x><y>-362.4760517957674</y></position><config><bulletinLevel>WARN</bulletinLevel><comments></comments><concurrentlySchedulableTaskCount>1</concurrentlySchedulableTaskCount><defaultConcurrentTasks><entry><key>TIMER_DRIVEN</key><value>1</value></entry><entry><key>EVENT_DRIVEN</key><value>0</value></entry><entry><key>CRON_DRIVEN</key><value>1</value></entry></defaultConcurrentTasks><defaultSchedulingPeriod><entry><key>TIMER_DRIVEN</key><value>0 sec</value></entry><entry><key>CRON_DRIVEN</key><value>* * * * * ?</value></entry></defaultSchedulingPeriod><descriptors><entry><key>Regular Expression</key><value><defaultValue>(?s:^.*$)</defaultValue><description>The Search Value to search for in the FlowFile content. Only used for 'Literal Replace' and 'Regex Replace' matching strategies</description><displayName>Search Value</displayName><dynamic>false</dynamic><name>Regular Expression</name><required>true</required><sensitive>false</sensitive><supportsEl>true</supportsEl></value></entry><entry><key>Replacement Value</key><value><defaultValue>$1</defaultValue><description>The value to insert using the 'Replacement Strategy'. Using &quot;Regex Replace&quot; back-references to Regular Expression capturing groups are supported, but back-references that reference capturing groups that do not exist in the regular expression will be treated as literal value. Back References may also be referenced using the Expression Language, as '$1', '$2', etc. The single-tick marks MUST be included, as these variables are not &quot;Standard&quot; attribute names (attribute names must be quoted unless they contain only numbers, letters, and _).</description><displayName>Replacement Value</displayName><dynamic>false</dynamic><name>Replacement Value</name><required>true</required><sensitive>false</sensitive><supportsEl>true</supportsEl></value></entry><entry><key>Character Set</key><value><defaultValue>UTF-8</defaultValue><description>The Character Set in which the file is encoded</description><displayName>Character Set</displayName><dynamic>false</dynamic><name>Character Set</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Maximum Buffer Size</key><value><defaultValue>1 MB</defaultValue><description>Specifies the maximum amount of data to buffer (per file or per line, depending on the Evaluation Mode) in order to apply the replacement. If 'Entire Text' (in Evaluation Mode) is selected and the FlowFile is larger than this value, the FlowFile will be routed to 'failure'. In 'Line-by-Line' Mode, if a single line is larger than this value, the FlowFile will be routed to 'failure'. A default value of 1 MB is provided, primarily for 'Entire Text' mode. In 'Line-by-Line' Mode, a value such as 8 KB or 16 KB is suggested. This value is ignored if the &lt;Replacement Strategy&gt; property is set to one of: Append, Prepend, Always Replace</description><displayName>Maximum Buffer Size</displayName><dynamic>false</dynamic><name>Maximum Buffer Size</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Replacement Strategy</key><value><allowableValues><description>Insert the Replacement Value at the beginning of the FlowFile or the beginning of each line (depending on the Evaluation Mode). For &quot;Line-by-Line&quot; Evaluation Mode, the value will be prepended to each line. For &quot;Entire Text&quot; evaluation mode, the value will be prepended to the entire text.</description><displayName>Prepend</displayName><value>Prepend</value></allowableValues><allowableValues><description>Insert the Replacement Value at the end of the FlowFile or the end of each line (depending on the Evluation Mode). For &quot;Line-by-Line&quot; Evaluation Mode, the value will be appended to each line. For &quot;Entire Text&quot; evaluation mode, the value will be appended to the entire text.</description><displayName>Append</displayName><value>Append</value></allowableValues><allowableValues><description>Interpret the Search Value as a Regular Expression and replace all matches with the Replacement Value. The Replacement Value may reference Capturing Groups used in the Search Value by using a dollar-sign followed by the Capturing Group number, such as $1 or $2. If the Search Value is set to .* then everything is replaced without even evaluating the Regular Expression.</description><displayName>Regex Replace</displayName><value>Regex Replace</value></allowableValues><allowableValues><description>Search for all instances of the Search Value and replace the matches with the Replacement Value.</description><displayName>Literal Replace</displayName><value>Literal Replace</value></allowableValues><allowableValues><description>Always replaces the entire line or the entire contents of the FlowFile (depending on the value of the &lt;Evaluation Mode&gt; property) and does not bother searching for any value. When this strategy is chosen, the &lt;Search Value&gt; property is ignored.</description><displayName>Always Replace</displayName><value>Always Replace</value></allowableValues><defaultValue>Regex Replace</defaultValue><description>The strategy for how and what to replace within the FlowFile's text content.</description><displayName>Replacement Strategy</displayName><dynamic>false</dynamic><name>Replacement Strategy</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Evaluation Mode</key><value><allowableValues><displayName>Line-by-Line</displayName><value>Line-by-Line</value></allowableValues><allowableValues><displayName>Entire text</displayName><value>Entire text</value></allowableValues><defaultValue>Entire text</defaultValue><description>Run the 'Replacement Strategy' against each line separately (Line-by-Line) or buffer the entire file into memory (Entire Text) and run against that.</description><displayName>Evaluation Mode</displayName><dynamic>false</dynamic><name>Evaluation Mode</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry></descriptors><lossTolerant>false</lossTolerant><penaltyDuration>30 sec</penaltyDuration><properties><entry><key>Regular Expression</key></entry><entry><key>Replacement Value</key><value>${now():format('yyyymmdd_hhmmss')}|</value></entry><entry><key>Character Set</key></entry><entry><key>Maximum Buffer Size</key></entry><entry><key>Replacement Strategy</key><value>Prepend</value></entry><entry><key>Evaluation Mode</key></entry></properties><runDurationMillis>0</runDurationMillis><schedulingPeriod>0 sec</schedulingPeriod><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><yieldDuration>1 sec</yieldDuration></config><name>Append Timestamp to Content</name><relationships><autoTerminate>true</autoTerminate><description>FlowFiles that could not be updated are routed to this relationship</description><name>failure</name></relationships><relationships><autoTerminate>false</autoTerminate><description>FlowFiles that have been successfully processed are routed to this relationship. This includes both FlowFiles that had text replaced and those that did not.</description><name>success</name></relationships><state>STOPPED</state><style/><supportsEventDriven>true</supportsEventDriven><supportsParallelProcessing>true</supportsParallelProcessing><type>org.apache.nifi.processors.standard.ReplaceText</type></processors><processors><id>f155df39-c188-43b1-bc24-a25145ed57df</id><parentGroupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</parentGroupId><position><x>-397.8943980771521</x><y>-38.653648218124374</y></position><config><annotationData>&lt;criteria&gt;
    &lt;flowFilePolicy&gt;USE_CLONE&lt;/flowFilePolicy&gt;
&lt;/criteria&gt;</annotationData><bulletinLevel>WARN</bulletinLevel><comments>
            </comments><concurrentlySchedulableTaskCount>1</concurrentlySchedulableTaskCount><defaultConcurrentTasks><entry><key>TIMER_DRIVEN</key><value>1</value></entry><entry><key>EVENT_DRIVEN</key><value>0</value></entry><entry><key>CRON_DRIVEN</key><value>1</value></entry></defaultConcurrentTasks><defaultSchedulingPeriod><entry><key>TIMER_DRIVEN</key><value>0 sec</value></entry><entry><key>CRON_DRIVEN</key><value>* * * * * ?</value></entry></defaultSchedulingPeriod><descriptors><entry><key>Delete Attributes Expression</key><value><description>Regular expression for attributes to be deleted from flowfiles.</description><displayName>Delete Attributes Expression</displayName><dynamic>false</dynamic><name>Delete Attributes Expression</name><required>false</required><sensitive>false</sensitive><supportsEl>true</supportsEl></value></entry><entry><key>filename</key><value><description></description><displayName>filename</displayName><dynamic>true</dynamic><name>filename</name><required>false</required><sensitive>false</sensitive><supportsEl>true</supportsEl></value></entry></descriptors><lossTolerant>false</lossTolerant><penaltyDuration>30 sec</penaltyDuration><properties><entry><key>Delete Attributes Expression</key></entry><entry><key>filename</key><value>${uuid}</value></entry></properties><runDurationMillis>0</runDurationMillis><schedulingPeriod>0 sec</schedulingPeriod><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><yieldDuration>1 sec</yieldDuration></config><name>GUID Filename</name><relationships><autoTerminate>false</autoTerminate><description>All FlowFiles are routed to this relationship</description><name>success</name></relationships><state>STOPPED</state><style/><supportsEventDriven>true</supportsEventDriven><supportsParallelProcessing>true</supportsParallelProcessing><type>org.apache.nifi.processors.attributes.UpdateAttribute</type></processors><processors><id>079d846d-d951-4188-9613-03e38083e238</id><parentGroupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</parentGroupId><position><x>-400.856927170168</x><y>118.58134959921074</y></position><config><bulletinLevel>WARN</bulletinLevel><comments>
            </comments><concurrentlySchedulableTaskCount>1</concurrentlySchedulableTaskCount><defaultConcurrentTasks><entry><key>TIMER_DRIVEN</key><value>1</value></entry><entry><key>EVENT_DRIVEN</key><value>0</value></entry><entry><key>CRON_DRIVEN</key><value>1</value></entry></defaultConcurrentTasks><defaultSchedulingPeriod><entry><key>TIMER_DRIVEN</key><value>0 sec</value></entry><entry><key>CRON_DRIVEN</key><value>* * * * * ?</value></entry></defaultSchedulingPeriod><descriptors><entry><key>Hadoop Configuration Resources</key><value><description>A file or comma separated list of files which contains the Hadoop file system configuration. Without this, Hadoop will search the classpath for a 'core-site.xml' and 'hdfs-site.xml' file or will revert to a default configuration.</description><displayName>Hadoop Configuration Resources</displayName><dynamic>false</dynamic><name>Hadoop Configuration Resources</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Kerberos Principal</key><value><description>Kerberos principal to authenticate as. Requires nifi.kerberos.krb5.file to be set in your nifi.properties</description><displayName>Kerberos Principal</displayName><dynamic>false</dynamic><name>Kerberos Principal</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Kerberos Keytab</key><value><description>Kerberos keytab associated with the principal. Requires nifi.kerberos.krb5.file to be set in your nifi.properties</description><displayName>Kerberos Keytab</displayName><dynamic>false</dynamic><name>Kerberos Keytab</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Kerberos Relogin Period</key><value><defaultValue>4 hours</defaultValue><description>Period of time which should pass before attempting a kerberos relogin</description><displayName>Kerberos Relogin Period</displayName><dynamic>false</dynamic><name>Kerberos Relogin Period</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Directory</key><value><description>The parent HDFS directory to which files should be written</description><displayName>Directory</displayName><dynamic>false</dynamic><name>Directory</name><required>true</required><sensitive>false</sensitive><supportsEl>true</supportsEl></value></entry><entry><key>Conflict Resolution Strategy</key><value><allowableValues><displayName>replace</displayName><value>replace</value></allowableValues><allowableValues><displayName>ignore</displayName><value>ignore</value></allowableValues><allowableValues><displayName>fail</displayName><value>fail</value></allowableValues><defaultValue>fail</defaultValue><description>Indicates what should happen when a file with the same name already exists in the output directory</description><displayName>Conflict Resolution Strategy</displayName><dynamic>false</dynamic><name>Conflict Resolution Strategy</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Block Size</key><value><description>Size of each block as written to HDFS. This overrides the Hadoop Configuration</description><displayName>Block Size</displayName><dynamic>false</dynamic><name>Block Size</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>IO Buffer Size</key><value><description>Amount of memory to use to buffer file contents during IO. This overrides the Hadoop Configuration</description><displayName>IO Buffer Size</displayName><dynamic>false</dynamic><name>IO Buffer Size</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Replication</key><value><description>Number of times that HDFS will replicate each file. This overrides the Hadoop Configuration</description><displayName>Replication</displayName><dynamic>false</dynamic><name>Replication</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Permissions umask</key><value><description>A umask represented as an octal number which determines the permissions of files written to HDFS. This overrides the Hadoop Configuration dfs.umaskmode</description><displayName>Permissions umask</displayName><dynamic>false</dynamic><name>Permissions umask</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Remote Owner</key><value><description>Changes the owner of the HDFS file to this value after it is written. This only works if NiFi is running as a user that has HDFS super user privilege to change owner</description><displayName>Remote Owner</displayName><dynamic>false</dynamic><name>Remote Owner</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Remote Group</key><value><description>Changes the group of the HDFS file to this value after it is written. This only works if NiFi is running as a user that has HDFS super user privilege to change group</description><displayName>Remote Group</displayName><dynamic>false</dynamic><name>Remote Group</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Compression codec</key><value><allowableValues><displayName>NONE</displayName><value>NONE</value></allowableValues><allowableValues><displayName>DEFAULT</displayName><value>DEFAULT</value></allowableValues><allowableValues><displayName>BZIP</displayName><value>BZIP</value></allowableValues><allowableValues><displayName>GZIP</displayName><value>GZIP</value></allowableValues><allowableValues><displayName>LZ4</displayName><value>LZ4</value></allowableValues><allowableValues><displayName>SNAPPY</displayName><value>SNAPPY</value></allowableValues><allowableValues><displayName>AUTOMATIC</displayName><value>AUTOMATIC</value></allowableValues><defaultValue>NONE</defaultValue><description></description><displayName>Compression codec</displayName><dynamic>false</dynamic><name>Compression codec</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry></descriptors><lossTolerant>false</lossTolerant><penaltyDuration>30 sec</penaltyDuration><properties><entry><key>Hadoop Configuration Resources</key><value>/usr/hdp/current/hadoop-client/conf/core-site.xml,/usr/hdp/current/hadoop-client/conf/hdfs-site.xml</value></entry><entry><key>Kerberos Principal</key></entry><entry><key>Kerberos Keytab</key></entry><entry><key>Kerberos Relogin Period</key></entry><entry><key>Directory</key><value>/</value></entry><entry><key>Conflict Resolution Strategy</key><value>fail</value></entry><entry><key>Block Size</key></entry><entry><key>IO Buffer Size</key></entry><entry><key>Replication</key></entry><entry><key>Permissions umask</key></entry><entry><key>Remote Owner</key></entry><entry><key>Remote Group</key></entry><entry><key>Compression codec</key></entry></properties><runDurationMillis>0</runDurationMillis><schedulingPeriod>0 sec</schedulingPeriod><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><yieldDuration>1 sec</yieldDuration></config><name>PutHDFS</name><relationships><autoTerminate>true</autoTerminate><description>Files that could not be written to HDFS for some reason are transferred to this relationship</description><name>failure</name></relationships><relationships><autoTerminate>true</autoTerminate><description>Files that have been successfully written to HDFS are transferred to this relationship</description><name>success</name></relationships><state>STOPPED</state><style/><supportsEventDriven>false</supportsEventDriven><supportsParallelProcessing>true</supportsParallelProcessing><type>org.apache.nifi.processors.hadoop.PutHDFS</type></processors><processors><id>298fca06-1fb3-441e-a155-f6074181d1cf</id><parentGroupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</parentGroupId><position><x>-392.23079214763175</x><y>-190.11086418254547</y></position><config><bulletinLevel>WARN</bulletinLevel><comments>NOTE: The demarcator here is set to \n in plaintext. (i.e I pressed Shift+Return to actually make a new line in the configuration area.)</comments><concurrentlySchedulableTaskCount>1</concurrentlySchedulableTaskCount><defaultConcurrentTasks><entry><key>TIMER_DRIVEN</key><value>1</value></entry><entry><key>EVENT_DRIVEN</key><value>0</value></entry><entry><key>CRON_DRIVEN</key><value>1</value></entry></defaultConcurrentTasks><defaultSchedulingPeriod><entry><key>TIMER_DRIVEN</key><value>0 sec</value></entry><entry><key>CRON_DRIVEN</key><value>* * * * * ?</value></entry></defaultSchedulingPeriod><descriptors><entry><key>Merge Strategy</key><value><allowableValues><description>Generates 'bins' of FlowFiles and fills each bin as full as possible. FlowFiles are placed into a bin based on their size and optionally their attributes (if the &lt;Correlation Attribute&gt; property is set)</description><displayName>Bin-Packing Algorithm</displayName><value>Bin-Packing Algorithm</value></allowableValues><allowableValues><description>Combines fragments that are associated by attributes back into a single cohesive FlowFile. If using this strategy, all FlowFiles must have the attributes &lt;fragment.identifier&gt;, &lt;fragment.count&gt;, and &lt;fragment.index&gt; or alternatively (for backward compatibility purposes) &lt;segment.identifier&gt;, &lt;segment.count&gt;, and &lt;segment.index&gt;. All FlowFiles with the same value for &quot;fragment.identifier&quot; will be grouped together. All FlowFiles in this group must have the same value for the &quot;fragment.count&quot; attribute. All FlowFiles in this group must have a unique value for the &quot;fragment.index&quot; attribute between 0 and the value of the &quot;fragment.count&quot; attribute.</description><displayName>Defragment</displayName><value>Defragment</value></allowableValues><defaultValue>Bin-Packing Algorithm</defaultValue><description>Specifies the algorithm used to merge content. The 'Defragment' algorithm combines fragments that are associated by attributes back into a single cohesive FlowFile. The 'Bin-Packing Algorithm' generates a FlowFile populated by arbitrarily chosen FlowFiles</description><displayName>Merge Strategy</displayName><dynamic>false</dynamic><name>Merge Strategy</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Merge Format</key><value><allowableValues><description>A bin of FlowFiles will be combined into a single TAR file. The FlowFiles' &lt;path&gt; attribute will be used to create a directory in the TAR file if the &lt;Keep Paths&gt; property is set to true; otherwise, all FlowFiles will be added at the root of the TAR file. If a FlowFile has an attribute named &lt;tar.permissions&gt; that is 3 characters, each between 0-7, that attribute will be used as the TAR entry's 'mode'.</description><displayName>TAR</displayName><value>TAR</value></allowableValues><allowableValues><description>A bin of FlowFiles will be combined into a single ZIP file. The FlowFiles' &lt;path&gt; attribute will be used to create a directory in the ZIP file if the &lt;Keep Paths&gt; property is set to true; otherwise, all FlowFiles will be added at the root of the ZIP file. The &lt;Compression Level&gt; property indicates the ZIP compression to use.</description><displayName>ZIP</displayName><value>ZIP</value></allowableValues><allowableValues><description>A bin of FlowFiles will be combined into a single Version 3 FlowFile Stream</description><displayName>FlowFile Stream, v3</displayName><value>FlowFile Stream, v3</value></allowableValues><allowableValues><description>A bin of FlowFiles will be combined into a single Version 2 FlowFile Stream</description><displayName>FlowFile Stream, v2</displayName><value>FlowFile Stream, v2</value></allowableValues><allowableValues><description>A bin of FlowFiles will be combined into a single Version 1 FlowFile Package</description><displayName>FlowFile Tar, v1</displayName><value>FlowFile Tar, v1</value></allowableValues><allowableValues><description>The contents of all FlowFiles will be concatenated together into a single FlowFile</description><displayName>Binary Concatenation</displayName><value>Binary Concatenation</value></allowableValues><allowableValues><description>The Avro contents of all FlowFiles will be concatenated together into a single FlowFile</description><displayName>Avro</displayName><value>Avro</value></allowableValues><defaultValue>Binary Concatenation</defaultValue><description>Determines the format that will be used to merge the content.</description><displayName>Merge Format</displayName><dynamic>false</dynamic><name>Merge Format</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Attribute Strategy</key><value><allowableValues><displayName>Keep Only Common Attributes</displayName><value>Keep Only Common Attributes</value></allowableValues><allowableValues><displayName>Keep All Unique Attributes</displayName><value>Keep All Unique Attributes</value></allowableValues><defaultValue>Keep Only Common Attributes</defaultValue><description>Determines which FlowFile attributes should be added to the bundle. If 'Keep All Unique Attributes' is selected, any attribute on any FlowFile that gets bundled will be kept unless its value conflicts with the value from another FlowFile. If 'Keep Only Common Attributes' is selected, only the attributes that exist on all FlowFiles in the bundle, with the same value, will be preserved.</description><displayName>Attribute Strategy</displayName><dynamic>false</dynamic><name>Attribute Strategy</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Correlation Attribute Name</key><value><description>If specified, like FlowFiles will be binned together, where 'like FlowFiles' means FlowFiles that have the same value for this Attribute. If not specified, FlowFiles are bundled by the order in which they are pulled from the queue.</description><displayName>Correlation Attribute Name</displayName><dynamic>false</dynamic><name>Correlation Attribute Name</name><required>false</required><sensitive>false</sensitive><supportsEl>true</supportsEl></value></entry><entry><key>Minimum Number of Entries</key><value><defaultValue>1</defaultValue><description>The minimum number of files to include in a bundle</description><displayName>Minimum Number of Entries</displayName><dynamic>false</dynamic><name>Minimum Number of Entries</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Maximum Number of Entries</key><value><description>The maximum number of files to include in a bundle. If not specified, there is no maximum.</description><displayName>Maximum Number of Entries</displayName><dynamic>false</dynamic><name>Maximum Number of Entries</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Minimum Group Size</key><value><defaultValue>0 B</defaultValue><description>The minimum size of for the bundle</description><displayName>Minimum Group Size</displayName><dynamic>false</dynamic><name>Minimum Group Size</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Maximum Group Size</key><value><description>The maximum size for the bundle. If not specified, there is no maximum.</description><displayName>Maximum Group Size</displayName><dynamic>false</dynamic><name>Maximum Group Size</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Max Bin Age</key><value><description>The maximum age of a Bin that will trigger a Bin to be complete. Expected format is &lt;duration&gt; &lt;time unit&gt; where &lt;duration&gt; is a positive integer and time unit is one of seconds, minutes, hours</description><displayName>Max Bin Age</displayName><dynamic>false</dynamic><name>Max Bin Age</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Maximum number of Bins</key><value><defaultValue>100</defaultValue><description>Specifies the maximum number of bins that can be held in memory at any one time</description><displayName>Maximum number of Bins</displayName><dynamic>false</dynamic><name>Maximum number of Bins</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Delimiter Strategy</key><value><allowableValues><description>The values of Header, Footer, and Demarcator will be retrieved from the contents of a file</description><displayName>Filename</displayName><value>Filename</value></allowableValues><allowableValues><description>The values of Header, Footer, and Demarcator will be specified as property values</description><displayName>Text</displayName><value>Text</value></allowableValues><defaultValue>Filename</defaultValue><description>Determines if Header, Footer, and Demarcator should point to files containing the respective content, or if the values of the properties should be used as the content.</description><displayName>Delimiter Strategy</displayName><dynamic>false</dynamic><name>Delimiter Strategy</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Header File</key><value><description>Filename specifying the header to use. If not specified, no header is supplied. This property is valid only when using the binary-concatenation merge strategy; otherwise, it is ignored.</description><displayName>Header</displayName><dynamic>false</dynamic><name>Header File</name><required>false</required><sensitive>false</sensitive><supportsEl>true</supportsEl></value></entry><entry><key>Footer File</key><value><description>Filename specifying the footer to use. If not specified, no footer is supplied. This property is valid only when using the binary-concatenation merge strategy; otherwise, it is ignored.</description><displayName>Footer</displayName><dynamic>false</dynamic><name>Footer File</name><required>false</required><sensitive>false</sensitive><supportsEl>true</supportsEl></value></entry><entry><key>Demarcator File</key><value><description>Filename specifying the demarcator to use. If not specified, no demarcator is supplied. This property is valid only when using the binary-concatenation merge strategy; otherwise, it is ignored.</description><displayName>Demarcator</displayName><dynamic>false</dynamic><name>Demarcator File</name><required>false</required><sensitive>false</sensitive><supportsEl>true</supportsEl></value></entry><entry><key>Compression Level</key><value><allowableValues><displayName>0</displayName><value>0</value></allowableValues><allowableValues><displayName>1</displayName><value>1</value></allowableValues><allowableValues><displayName>2</displayName><value>2</value></allowableValues><allowableValues><displayName>3</displayName><value>3</value></allowableValues><allowableValues><displayName>4</displayName><value>4</value></allowableValues><allowableValues><displayName>5</displayName><value>5</value></allowableValues><allowableValues><displayName>6</displayName><value>6</value></allowableValues><allowableValues><displayName>7</displayName><value>7</value></allowableValues><allowableValues><displayName>8</displayName><value>8</value></allowableValues><allowableValues><displayName>9</displayName><value>9</value></allowableValues><defaultValue>1</defaultValue><description>Specifies the compression level to use when using the Zip Merge Format; if not using the Zip Merge Format, this value is ignored</description><displayName>Compression Level</displayName><dynamic>false</dynamic><name>Compression Level</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Keep Path</key><value><allowableValues><displayName>true</displayName><value>true</value></allowableValues><allowableValues><displayName>false</displayName><value>false</value></allowableValues><defaultValue>false</defaultValue><description>If using the Zip or Tar Merge Format, specifies whether or not the FlowFiles' paths should be included in their entry names; if using other merge strategy, this value is ignored</description><displayName>Keep Path</displayName><dynamic>false</dynamic><name>Keep Path</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry></descriptors><lossTolerant>false</lossTolerant><penaltyDuration>30 sec</penaltyDuration><properties><entry><key>Merge Strategy</key></entry><entry><key>Merge Format</key></entry><entry><key>Attribute Strategy</key></entry><entry><key>Correlation Attribute Name</key></entry><entry><key>Minimum Number of Entries</key></entry><entry><key>Maximum Number of Entries</key></entry><entry><key>Minimum Group Size</key><value>500 B</value></entry><entry><key>Maximum Group Size</key></entry><entry><key>Max Bin Age</key></entry><entry><key>Maximum number of Bins</key><value>100000</value></entry><entry><key>Delimiter Strategy</key><value>Text</value></entry><entry><key>Header File</key></entry><entry><key>Footer File</key></entry><entry><key>Demarcator File</key><value>
</value></entry><entry><key>Compression Level</key></entry><entry><key>Keep Path</key></entry></properties><runDurationMillis>0</runDurationMillis><schedulingPeriod>0 sec</schedulingPeriod><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><yieldDuration>1 sec</yieldDuration></config><name>Hold Flow until Suitable Size</name><relationships><autoTerminate>true</autoTerminate><description>If the bundle cannot be created, all FlowFiles that would have been used to created the bundle will be transferred to failure</description><name>failure</name></relationships><relationships><autoTerminate>false</autoTerminate><description>The FlowFile containing the merged content</description><name>merged</name></relationships><relationships><autoTerminate>true</autoTerminate><description>The FlowFiles that were used to create the bundle</description><name>original</name></relationships><state>STOPPED</state><style/><supportsEventDriven>false</supportsEventDriven><supportsParallelProcessing>true</supportsParallelProcessing><type>org.apache.nifi.processors.standard.MergeContent</type></processors><processors><id>a2df0212-3e79-4d31-8dfd-3271bef4cbe0</id><parentGroupId>5ddcd9a9-daa3-47a4-9009-aaf892e3a601</parentGroupId><position><x>-395.38626097451237</x><y>-525.0499498225602</y></position><config><bulletinLevel>WARN</bulletinLevel><comments>Pull from kafka.Required fields here are Topic and Zookeeper connection string. Everything else should be tweaked as needed.</comments><concurrentlySchedulableTaskCount>1</concurrentlySchedulableTaskCount><defaultConcurrentTasks><entry><key>TIMER_DRIVEN</key><value>1</value></entry><entry><key>EVENT_DRIVEN</key><value>0</value></entry><entry><key>CRON_DRIVEN</key><value>1</value></entry></defaultConcurrentTasks><defaultSchedulingPeriod><entry><key>TIMER_DRIVEN</key><value>0 sec</value></entry><entry><key>CRON_DRIVEN</key><value>* * * * * ?</value></entry></defaultSchedulingPeriod><descriptors><entry><key>ZooKeeper Connection String</key><value><description>The Connection String to use in order to connect to ZooKeeper. This is often a comma-separated list of &lt;host&gt;:&lt;port&gt; combinations. For example, host1:2181,host2:2181,host3:2188</description><displayName>ZooKeeper Connection String</displayName><dynamic>false</dynamic><name>ZooKeeper Connection String</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Topic Name</key><value><description>The Kafka Topic to pull messages from</description><displayName>Topic Name</displayName><dynamic>false</dynamic><name>Topic Name</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Zookeeper Commit Frequency</key><value><defaultValue>60 secs</defaultValue><description>Specifies how often to communicate with ZooKeeper to indicate which messages have been pulled. A longer time period will result in better overall performance but can result in more data duplication if a NiFi node is lost</description><displayName>Zookeeper Commit Frequency</displayName><dynamic>false</dynamic><name>Zookeeper Commit Frequency</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Batch Size</key><value><defaultValue>1</defaultValue><description>Specifies the maximum number of messages to combine into a single FlowFile. These messages will be concatenated together with the &lt;Message Demarcator&gt; string placed between the content of each message. If the messages from Kafka should not be concatenated together, leave this value at 1.</description><displayName>Batch Size</displayName><dynamic>false</dynamic><name>Batch Size</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Message Demarcator</key><value><defaultValue>\n</defaultValue><description>Specifies the characters to use in order to demarcate multiple messages from Kafka. If the &lt;Batch Size&gt; property is set to 1, this value is ignored. Otherwise, for each two subsequent messages in the batch, this value will be placed in between them.</description><displayName>Message Demarcator</displayName><dynamic>false</dynamic><name>Message Demarcator</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Client Name</key><value><defaultValue>NiFi-a2df0212-3e79-4d31-8dfd-3271bef4cbe0</defaultValue><description>Client Name to use when communicating with Kafka</description><displayName>Client Name</displayName><dynamic>false</dynamic><name>Client Name</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Group ID</key><value><defaultValue>a2df0212-3e79-4d31-8dfd-3271bef4cbe0</defaultValue><description>A Group ID is used to identify consumers that are within the same consumer group</description><displayName>Group ID</displayName><dynamic>false</dynamic><name>Group ID</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Kafka Communications Timeout</key><value><defaultValue>30 secs</defaultValue><description>The amount of time to wait for a response from Kafka before determining that there is a communications error</description><displayName>Kafka Communications Timeout</displayName><dynamic>false</dynamic><name>Kafka Communications Timeout</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>ZooKeeper Communications Timeout</key><value><defaultValue>30 secs</defaultValue><description>The amount of time to wait for a response from ZooKeeper before determining that there is a communications error</description><displayName>ZooKeeper Communications Timeout</displayName><dynamic>false</dynamic><name>ZooKeeper Communications Timeout</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Auto Offset Reset</key><value><allowableValues><displayName>smallest</displayName><value>smallest</value></allowableValues><allowableValues><displayName>largest</displayName><value>largest</value></allowableValues><defaultValue>largest</defaultValue><description>Automatically reset the offset to the smallest or largest offset available on the broker</description><displayName>Auto Offset Reset</displayName><dynamic>false</dynamic><name>Auto Offset Reset</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry></descriptors><lossTolerant>false</lossTolerant><penaltyDuration>30 sec</penaltyDuration><properties><entry><key>ZooKeeper Connection String</key><value>sandbox.hortonworks.com:2181</value></entry><entry><key>Topic Name</key><value>test-datetime</value></entry><entry><key>Zookeeper Commit Frequency</key></entry><entry><key>Batch Size</key></entry><entry><key>Message Demarcator</key></entry><entry><key>Client Name</key><value>client_name</value></entry><entry><key>Group ID</key><value>group_id</value></entry><entry><key>Kafka Communications Timeout</key></entry><entry><key>ZooKeeper Communications Timeout</key></entry><entry><key>Auto Offset Reset</key></entry></properties><runDurationMillis>0</runDurationMillis><schedulingPeriod>0 sec</schedulingPeriod><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><yieldDuration>1 sec</yieldDuration></config><name>GetKafka</name><relationships><autoTerminate>false</autoTerminate><description>All FlowFiles that are created are routed to this relationship</description><name>success</name></relationships><state>STOPPED</state><style/><supportsEventDriven>false</supportsEventDriven><supportsParallelProcessing>true</supportsParallelProcessing><type>org.apache.nifi.processors.kafka.GetKafka</type></processors></contents><disabledCount>0</disabledCount><inactiveRemotePortCount>0</inactiveRemotePortCount><inputPortCount>0</inputPortCount><invalidCount>0</invalidCount><name>KafkaRawToHDFS</name><outputPortCount>0</outputPortCount><parent><id>7c84501d-d10c-407c-b9f3-1d80e38fe36a</id><name>NiFi Flow</name></parent><runningCount>0</runningCount><stoppedCount>5</stoppedCount></processGroups></snippet><timestamp>06/06/2016 22:35:20 UTC</timestamp></template>